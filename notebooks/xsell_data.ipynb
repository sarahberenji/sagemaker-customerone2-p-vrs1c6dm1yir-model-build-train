{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws codeartifact login --tool pip --domain cirrus-ml-ds-domain --domain-owner 813736554012 --repository cirrus-ml-ds-shared-repo\n",
    "!pip install awswrangler --quiet\n",
    "!pip install lightgbm --quiet\n",
    "!pip install category_encoders --quiet\n",
    "!pip install imbalanced-learn --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import awswrangler as wr\n",
    "import pandas as pd\n",
    "from lightgbm import LGBMClassifier\n",
    "import numpy as np\n",
    "\n",
    "boto3.setup_default_session(region_name=\"eu-north-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = wr.athena.read_sql_query('SELECT * FROM \"customerone_mock_data_rl\".\"master\";', \n",
    "                              database=\"customerone_mock_data_rl\",\n",
    "                              workgroup=\"dev-athena-workgroup\",\n",
    "                             )\n",
    "#                               workgroup=\"dev-ds-athena-workgroup\",\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 74 columns\n",
    "# [print(f\"\\n** {col}:\\n{df[col].value_counts(dropna=False)} \\n\") for col in df.columns];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0- configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col_name = 'tgt_xsell_cust_voice_to_fixed'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1- sample_training_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training = df[df['tgt_xsell_cust_voice_to_fixed'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop_invalid_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 & 8- splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {{model}}.training.splitting_params:\n",
    "#   splitter: customerone.pipelines.telco.test_train_splitter.OutOfTimeSplitter\n",
    "#   splitter_args: # Arguments to be passed into the splitter, see the splitter docs for details\n",
    "#     n_splits: 1 # The n_splits argument controls the number of iterations\n",
    "#     test_size: 0.2\n",
    "#     date_col: \"current_dt\"\n",
    "#     verbose: 1\n",
    "#     random_state: 42\n",
    "#   group_col_name: \"customer_id\" # column be useded as a group vector, if the selected spliter uses the group parameter. See the splitter docs for details\n",
    "#   iteration_col_name: iteration_id # Name of the iteration ID column to be added\n",
    "#   split_col_name: split # Name of the TRAIN/TEST/etc. split column\n",
    "\n",
    "# {{model}}.training.subsplitting_params:\n",
    "#   splitter: sklearn.model_selection.StratifiedShuffleSplit\n",
    "#   splitter_args: # Arguments to be passed into the splitter, see the splitter docs for details\n",
    "#     test_size: 0.1\n",
    "#     random_state: 42 # n_splits is fixed to be 1 for sub-splitters. Any higher value will be ignored\n",
    "#   source_split: TRAIN # The set to split\n",
    "#   target_splits: # The two sets to split the source set into. Must be two targets\n",
    "#     - TRAIN\n",
    "#     - CAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Added to Sagemaker\n",
    "\n",
    "from utils_split import make_splits, split_data, make_subsplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Added to Sagemaker\n",
    "split_train_test = make_splits(df_training_idx, target_col_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{df_training.shape}, {df_training_idx.shape}, {split_train_test.shape}\")\n",
    "# (5896, 74), (5896, 76), (5896, 76)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_train_test['split'].value_counts(dropna=False)\n",
    "# TRAIN    4654\n",
    "# TEST     1242\n",
    "# Name: split, dtype: int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training_idx['split'].value_counts(dropna=False)\n",
    "# NaN    5896\n",
    "# Name: split, dtype: int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_train_test['iteration_id'].value_counts(dropna=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Added to Sagemaker\n",
    "\n",
    "splitter_args = {'test_size': 0.1, 'random_state': 42}\n",
    "subsplit_params = {\"iteration_col_name\": 'iteration_id', \"split_col_name\": \"split\", 'source_split': 'TRAIN', 'target_splits':['TRAIN', 'CAL']}\n",
    "\n",
    "split_train_test_cal = make_subsplit(split_train_test, subsplit_params, splitter_args, target_col_name)\n",
    "split_train_test_cal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split_train_test['split'].value_counts(dropna=False)\n",
    "# TRAIN    4654\n",
    "# TEST     1242\n",
    "split_train_test_cal['split'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7- feature types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# added to sagemaker\n",
    "from utils_determine_feature_type import determine_feature_data_types\n",
    "\n",
    "spine_params_determine_feature_data_types = {'keys': ['customer_id'], \n",
    "                'date_column': 'current_dt', \n",
    "                'product_holdings_filter': {'product_category': 'fixedbroadband'}, \n",
    "                'is_deepsell': 'N'}\n",
    "\n",
    "target_params_determine_feature_data_types = {'target_variable_column': 'tgt_xsell_cust_voice_to_fixed'\n",
    "                 , 'lead_time_window': '1d'\n",
    "                 , 'target_window': '45d'\n",
    "                 , 'product_activation_filter': {'product_sub_category': 'voice'}\n",
    "                 , 'campaign_keys': ['customer_id']\n",
    "                 , 'campaign_filter': {'campaign_name':[\"C-452-O-06 Korsförsäljning Telia Life 2.0 - Sälja mobilt\"\n",
    "                                            , \"b2c_cross-sell_pp_crossSellPpToBb\"\n",
    "                                            , \"b2c_cross-sell_pp_crossSellPpToBb_oldContent\"\n",
    "                                            , \"b2c_cross-sell_pp_crossSellPpToBb_REM1\"\n",
    "                                            , \"C-700-O-03 Cross-sell Mobile to Broadband customers (TM only)\"\n",
    "                                            , \"C-700-O-01 Cross-sell Mobile to Broadband customers (A)\"\n",
    "                                            , \"C-752-O GEOF 2021 - X-sell PP\"\n",
    "                                            , \"C-652-O Black Friday Erbjudande 2019  Mobilt till BB-kund - activity 1\"\n",
    "                                            , \"C-700-O-02 Cross-sell Mobile to Broadband customers (B)\"\n",
    "                                            , \"b2c_cross-sell_Pp_PpToBb_default\"\n",
    "                                            , \"C-752-O GEOF 2021 - Xsell PP\"\n",
    "                                            , \"b2c_cross-sell_Pp_PpToBb_simOnly\"\n",
    "                                            , \"b2c_cross-sell_Pp_PpToBb_samS215G\"\n",
    "                                            , \"b2c_cross-sell_Pp_PpToBb_iphone12Mini\"\n",
    "                                            , \"b2c_cross-sell_Pp_PpToBb_iphoneSE\"\n",
    "                                            , \"b2c_cross-sell_Pp_PpToBb_iphone12\"\n",
    "                                            , \"b2c_cross-sell_Pp_PpToBb_samS20FE5G\"\n",
    "                                            , \"b2c_cross-sell_Pp_PpToBb_sonyXp10lll\"\n",
    "                                            , \"b2c_cross-sell_Pp_PpToBb_default_short_8pm\"\n",
    "                                            , \"b2c_cross-sell_Pp_PpToBb_default_8pm\"]\n",
    "                                       , 'customer_actioned_flg_column': {'Email':'actioned_ind'}}}\n",
    "                 \n",
    "\n",
    "feature_dict = determine_feature_data_types(df_training, spine_params_determine_feature_data_types, target_params_determine_feature_data_types)\n",
    "print(f\"len(feature_dict)={len(feature_dict)}, \\nlen(feature_dict['numeric'])={len(feature_dict['numeric'])}, \\nlen(feature_dict['categorical'])={len(feature_dict['categorical'])}\")\n",
    "feature_dict\n",
    "# actual data: len(categorical)=7 , len(numerical)=150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10- preprocessing_pipeline_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#                 node(\n",
    "#                     func=preprocessing_pipeline_step,\n",
    "#                     inputs={\n",
    "#                         \"params\": f\"{param_str}.preprocessing\",\n",
    "#                         \"categorical\": f\"{model_name}.in_memory.categorical_features\",\n",
    "#                         \"numerical\": f\"{model_name}.in_memory.numerical_features\",\n",
    "#                     },\n",
    "#                     outputs=f\"{catalog_str}.preprocessing_step\",\n",
    "#                     name=f\"define_preprocessing\",\n",
    "#                     tags=[\"training: model\"],\n",
    "#                 )\n",
    "            \n",
    "\n",
    "# categorical_features = {'categorical': {'impute': {'class': 'customerone_lib.common.pkgs.predictive_modeling.preprocessing.PandasSimpleImputer',\n",
    "#                                                     'args': {'strategy': 'most_frequent'}},\n",
    "#                                         'encoder': {'class': 'category_encoders.target_encoder.TargetEncoder'}}}\n",
    "\n",
    "# numerical_features = {'numerical':{'imputer': {'class': 'customerone_lib.common.pkgs.predictive_modeling.preprocessing.PandasSimpleImputer',\n",
    "#                                                'args': {'strategy': 'mean'}}}}\n",
    "\n",
    "# params_preprocessing = {'categorical': {'impute': PandasSimpleImputer(strategy='most_frequent'),\n",
    "#                                         'encoder': TargetEncoder()}, \n",
    "#                         'numerical': {'imputer': PandasSimpleImputer()}}\n",
    "\n",
    "%load_ext autoreload \n",
    "%autoreload 2\n",
    "from utiles_preprocessing import preprocessing_pipeline_step\n",
    "\n",
    "\n",
    "preprocessing_step = preprocessing_pipeline_step(feature_dict['categorical'], feature_dict['numeric'])\n",
    "preprocessing_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# before vs after imputation?!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11 & 2 estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "# from sklearn.pipeline import Pipeline\n",
    "\n",
    "def model_estimator(**kwargs):\n",
    "# def model_estimator(params):\n",
    "# def model_estimator(params: Dict[str, Any],) -> BaseEstimator:\n",
    "    \"\"\"\n",
    "    Define a estimator\n",
    "\n",
    "    `params` expects the following key value pairs:\n",
    "        `estimator`: path to estimator class\n",
    "        `args`: (Optional) arguments to be passed when initiating the estimator object\n",
    "\n",
    "    or \n",
    "\n",
    "    `params` expects the following key value pairs:\n",
    "        `pipeline`: path to pipeline class\n",
    "        `args`: arguments to be passed when initiating the pipeline object.\n",
    "            Must at least contain the `steps` key\n",
    "            `steps`: steps defining transformers (implementing fit/transform/fit_resample), \n",
    "                in the order in which they are chained, with the last object being an estimator.  \n",
    "                `step_name_1`: name of the first step\n",
    "                    `transformer`: Path to transformer/estimator class\n",
    "                    `args`: arguments to be passed to transformer/estimator instantiation\n",
    "                `step_name_2`: name of the second step, follows the same structure as step one\n",
    "                    ...\n",
    "\n",
    "    Args:\n",
    "        params: dictonary with the keys {'estimator': string, 'args': dict} \n",
    "            or {'pipeline': string, 'args': {'steps': dict}}\n",
    "\n",
    "    Returns:\n",
    "        estimator\n",
    "    \"\"\"\n",
    "\n",
    "    estimator_pipeline = Pipeline(steps=[('over_sampler', SMOTE(random_state=42)),\n",
    "                                         ('estimator', LGBMClassifier())])\n",
    "    if len(kwargs):\n",
    "        estimator_pipeline.steps.insert(kwargs.get('index'), (kwargs.get('step_name'), kwargs.get('step')))\n",
    "    return estimator_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimator_params = {'pipeline': 'imblearn.pipeline.Pipeline', \n",
    "#                     'args': {'steps': {'over_sampler': {'class': 'imblearn.over_sampling.SMOTE', \n",
    "#                                                         'args': {'random_state': 42}}, \n",
    "#                                        'estimator': {'class': 'lightgbm.LGBMClassifier'}}}}\n",
    "add_step = {'step':preprocessing_step, 'index': 0, 'step_name':'preprocessing'}\n",
    "estimator = model_estimator(**add_step)\n",
    "estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   estimator = Pipeline(steps=[('preprocessing',\n",
    "#                  ColumnTransformer(transformers=[('categorical',\n",
    "#                                                   Pipeline(steps=[('impute',\n",
    "#                                                                    PandasSimpleImputer(strategy='most_frequent')),\n",
    "#                                                                   ('encoder',\n",
    "#                                                                    TargetEncoder())]),\n",
    "#                                                   ['dmgrphc_b_gender_typ',\n",
    "#                                                    'dmgrphc_b_habits_desc_txt',\n",
    "#                                                    'dmgrphc_b_age_bucket_txt',\n",
    "#                                                    'dmgrphc_b_city_txt',\n",
    "#                                                    'dmgrphc_b_lifestyle_desc_txt',\n",
    "#                                                    'rev_m_bill_shock_eom_total_bill_amt...\n",
    "#                                                    'product_holding_mobilebroadbandsubscription_active_30_to_60_days_avg',\n",
    "#                                                    'product_holding_fixedbroadband_active_0_to_30_over_30_to_60_days_avg',\n",
    "#                                                    'ci_d_sum_cmpl_over_contacts_in_0_30_days',\n",
    "#                                                    'product_holding_fixedbroadband_closed_0_to_30_over_30_to_60_days_avg',\n",
    "#                                                    'num_distinct_campaign_last_30_days', ...])])),\n",
    "#                 ('over_sampler', SMOTE(random_state=42)),\n",
    "#                 ('estimator', LGBMClassifier())])   \n",
    "\n",
    "\n",
    "# node(\n",
    "#     func=model_estimator,\n",
    "#     inputs={\n",
    "#         \"params\": f\"{param_str}.estimator\",\n",
    "#     },\n",
    "#     outputs=f\"{model_name}.in_memory.estimator\",\n",
    "#     name=f\"define_estimator\",\n",
    "#     tags=[\"training: model\"],\n",
    "# ),\n",
    "# node(\n",
    "#     func=partial(add_step, step_name=\"preprocessing\"),\n",
    "#     inputs={\n",
    "#         \"pipeline\": f\"{model_name}.in_memory.estimator\",\n",
    "#         \"step\": f\"{catalog_str}.preprocessing_step\"\n",
    "#     },\n",
    "#     outputs=f\"{catalog_str}.estimator\",\n",
    "#     name=f\"add_preprocessing_step_to_estimator\",\n",
    "#     tags=[\"training: model\"],\n",
    "# )\n",
    "        \n",
    "        \n",
    "# {{model}}.training.estimator:\n",
    "#   pipeline: imblearn.pipeline.Pipeline\n",
    "#   args:\n",
    "#     steps:\n",
    "#       over_sampler: \n",
    "#         class: imblearn.over_sampling.SMOTE\n",
    "#         args:\n",
    "#           random_state: 42\n",
    "#       estimator: \n",
    "#         class: lightgbm.LGBMClassifier        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12- tune_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# line features is already added to sagemaker!!\n",
    "\n",
    "%load_ext autoreload \n",
    "%autoreload 2\n",
    "from utils_tune_parameters import tune_parameters\n",
    "\n",
    "\n",
    "tuning_params = {'tuner': 'sklearn.model_selection.RandomizedSearchCV', \n",
    "                 'tuner_args': {'scoring': 'roc_auc', 'verbose': 2, 'n_iter': 10, \n",
    "                                'param_distributions': {'preprocessing__categorical__encoder__min_samples_leaf': [100, 500, 1000, 5000, 10000],\n",
    "                                                         'preprocessing__categorical__encoder__smoothing': [1.0, 5.0, 10.0, 25.0], \n",
    "                                                         'over_sampler__sampling_strategy': [0.5, 0.35, 0.25, 0.15, 0.1, 'not minority'], \n",
    "                                                         'estimator__boosting_type': ['gbdt', 'dart'], \n",
    "                                                         'estimator__num_leaves': [20, 31, 50, 100], \n",
    "                                                         'estimator__min_child_samples': [100, 250, 500, 1000], \n",
    "                                                         'estimator__min_child_weight': [0.01, 0.001], \n",
    "                                                         'estimator__max_depth': [5, 10, 15, -1], \n",
    "                                                         'estimator__learning_rate': [0.2, 0.15, 0.1, 0.05, 0.01], \n",
    "                                                         'estimator__n_estimators': [50, 100, 150], \n",
    "                                                         'estimator__colsample_bytree': [0.8, 1.0], \n",
    "                                                         'estimator__subsample': [0.8, 1.0], \n",
    "                                                         'estimator__subsample_freq': [1], \n",
    "                                                         'estimator__reg_alpha': [0.0, 0.1],\n",
    "                                                         'estimator__reg_lambda': [0.0, 0.1],\n",
    "                                                         'estimator__min_split_gain': [0.0, 0.1],\n",
    "                                                         'estimator__class_weight': [\"balanced\", []],\n",
    "                                                        }}, \n",
    "                 'cv': 'test_train_splitter.OutOfTimeSplitter', \n",
    "                 'cv_args': {'n_splits': 3, 'test_size': 0.45, 'date_col': 'current_dt', 'random_state': 42, 'verbose': 1}}\n",
    "\n",
    "features = feature_dict['categorical'] + feature_dict['numeric']\n",
    "\n",
    "splitting_params = {'splitter': 'test_train_splitter.OutOfTimeSplitter', \n",
    "                    'splitter_args': {'n_splits': 1, 'test_size': 0.2, 'date_col': 'current_dt', 'verbose': 1, 'random_state': 42}, \n",
    "                    'group_col_name': 'customer_id', \n",
    "                    'iteration_col_name': 'iteration_id', \n",
    "                    'split_col_name': 'split'}\n",
    "\n",
    "target = 'tgt_xsell_cust_voice_to_fixed'\n",
    "\n",
    "tuned_parameters, best_tuning_score = tune_parameters(split_train_test_cal, estimator, tuning_params, features, splitting_params, target_col_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# node(\n",
    "#     func=tune_parameters,\n",
    "#     inputs={\n",
    "#         \"data\": f\"{catalog_str}.split_train_test_cal\",\n",
    "#         \"estimator\": f\"{catalog_str}.estimator\",\n",
    "#         \"tuning_params\": f\"{param_str}.tuning_params\",\n",
    "#         \"features\": f\"{model_name}.in_memory.features\",\n",
    "#         \"splitting_params\": f\"{param_str}.splitting_params\",\n",
    "#         \"target_col_name\": f\"{param_str}.target\",\n",
    "#     },\n",
    "#     outputs=[\n",
    "#         f\"{catalog_str}.tuned_parameters\",\n",
    "#         f\"{catalog_str}.best_tuning_score\"\n",
    "#     ],\n",
    "#     name=f\"tune_model_params_for_training\",\n",
    "#     tags=[\"training: model\"],\n",
    "# )\n",
    "    \n",
    "    \n",
    "# # No features are specified for this model as they are found automatically in feature selection\n",
    "# {{model}}.training.tuning_params:\n",
    "#   tuner: sklearn.model_selection.RandomizedSearchCV\n",
    "#   tuner_args:\n",
    "#     scoring: roc_auc\n",
    "#     verbose: 2\n",
    "#     n_iter: 10\n",
    "#     # if estimator is a sklearn/imblearn pipeline, then the param_distributions \n",
    "#     # needs to be prefixed with step name, e.g '{step_name}__{parameter}'\n",
    "#     param_distributions:\n",
    "#       preprocessing__categorical__encoder__min_samples_leaf: [100, 500, 1000, 5000, 10000]\n",
    "#       preprocessing__categorical__encoder__smoothing: [1.0, 5.0, 10.0, 25.0] # float is needed\n",
    "#       # 'not minority' acts as a 'passthrough' for SMOTE\n",
    "#       over_sampler__sampling_strategy: [0.5, 0.35, 0.25, 0.15, 0.1, \"not minority\"]\n",
    "#       estimator__boosting_type: [\"gbdt\", \"dart\"]\n",
    "#       estimator__num_leaves: [20, 31, 50, 100]\n",
    "#       estimator__min_child_samples: [100, 250, 500, 1000]\n",
    "#       estimator__min_child_weight: [0.01, 0.001]\n",
    "#       estimator__max_depth: [5, 10, 15, -1]\n",
    "#       estimator__learning_rate: [0.2, 0.15, 0.1, 0.05, 0.01]\n",
    "#       estimator__n_estimators: [50, 100, 150]\n",
    "#       estimator__colsample_bytree: [0.8, 1.0]\n",
    "#       estimator__subsample: [0.8, 1.0]\n",
    "#       estimator__subsample_freq: [1]\n",
    "#       estimator__reg_alpha: [0.0, 0.1]\n",
    "#       estimator__reg_lambda: [0.0, 0.1]\n",
    "#       estimator__min_split_gain: [0.0, 0.1]\n",
    "#       estimator__class_weight: [\"balanced\", []] # empty list [] acts as None, all classes gets weight one\n",
    "#   cv: customerone.pipelines.telco.test_train_splitter.OutOfTimeSplitter\n",
    "#   cv_args:’”‘\n",
    "#     n_splits: 3\n",
    "#     test_size: 0.45\n",
    "#     date_col: current_dt\n",
    "#     random_state: 42\n",
    "#     verbose: 1    \n",
    "\n",
    "        \n",
    "        \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_tuning_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13- fit_model_for_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload \n",
    "%autoreload 2\n",
    "from utils_train_model import train_model\n",
    "\n",
    "\n",
    "if len(tuned_parameters) > 1:\n",
    "    ValueError(\"Sarah: There are more than one tuned_parameters available!!!! Can't start train_model()\")\n",
    "fit_models = train_model(split_train_test_cal, tuned_parameters[0], features, splitting_params, target_col_name, estimator)\n",
    "fit_models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# node(\n",
    "#     func=train_model,\n",
    "#     inputs={\n",
    "#         \"data\": f\"{catalog_str}.split_train_test_cal\",\n",
    "#         \"estimator\": f\"{catalog_str}.estimator\",\n",
    "#         \"model_params\": f\"{catalog_str}.tuned_parameters\",\n",
    "#         \"features\": f\"{model_name}.in_memory.features\",\n",
    "#         \"splitting_params\": f\"{param_str}.splitting_params\",\n",
    "#         \"target_col_name\": f\"{param_str}.target\",\n",
    "#     },\n",
    "#     outputs=f\"{catalog_str}.fit_models\",\n",
    "#     name=f\"fit_model_for_training\",\n",
    "#     tags=[\"training: model\"],\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(fit_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14- calibrate classifires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.pipeline import Pipeline as imblearn_pipeline\n",
    "from sklearn.pipeline import Pipeline as sklearn_pipeline\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "def calibrate_classifiers(data, trained_models, splitting_params, calibration_params, target_col_name):\n",
    "# def calibrate_classifiers(data: pd.DataFrame, trained_models: Mapping[str, Any], splitting_params: Mapping[str, Any], calibration_params: Optional[Mapping[str, Any]] = None,) -> Mapping[str, Any]:\n",
    "    \"\"\"Calibrates prefit sklearn compatible models using `CalibratedClassifierCV`.\n",
    "\n",
    "    `splitting_params` expects the following key value pairs:\n",
    "        `iteration_col_name`: Name of column in which the fold id is to be saved\n",
    "        `split_col_name`: Name of column in which the TRAIN/TEST split is saved\n",
    "\n",
    "    `calibration_params` may be used with following optional key value pairs:\n",
    "        `method`: (Optional) Calibration method, `sigmoid` or `isotonic`. Default is\n",
    "            `sigmoid`\n",
    "        `calibration_split_name`: (Optional) Split on which model will be calibrated,\n",
    "            default is `CAL`\n",
    "\n",
    "    Args:\n",
    "        data: A dataframe containing features, targets, fold and split information\n",
    "        trained_models: Prefit sklearn compatible classifiers\n",
    "        splitting_params: Describe where fold and splitting information are located\n",
    "        calibration_params: Describe which method and data to use\n",
    "\n",
    "    Returns:\n",
    "        A dictionary with the fold id as key and the model calibrated on that fold\n",
    "            as value\n",
    "    \"\"\"\n",
    "    calibration_params = {} if calibration_params is None else calibration_params\n",
    "    iteration_col_name = splitting_params.get(\"iteration_col_name\", \"iteration_id\")\n",
    "    split_col_name = splitting_params.get(\"split_col_name\", \"split\")\n",
    "\n",
    "    calibration_method = calibration_params.get(\"method\", \"sigmoid\")\n",
    "    calibration_split_name = calibration_params.get(\"calibration_split_name\", \"CAL\")\n",
    "#     print(f\"  calibration_params={calibration_params}, \\n  iteration_col_name={iteration_col_name}, \\n  split_col_name={split_col_name}, \\n  calibration_method={calibration_method}, \\n  calibration_split_name={calibration_split_name}\")\n",
    "\n",
    "    calibrated_model = data.groupby(iteration_col_name).apply(\n",
    "                                _calibrate_models_per_fold,\n",
    "                                trained_models,\n",
    "                                calibration_method,\n",
    "                                calibration_split_name,\n",
    "                                split_col_name,\n",
    "                                ).to_dict()\n",
    "#     print(f\"calibrated_model={calibrated_model}\")\n",
    "    for k in calibrated_model:\n",
    "#         print(f\"\\t k = {k}\")\n",
    "        trained_models[k]['calibrated_model'] = calibrated_model[k]\n",
    "    print(f\"len(trained_models) = {len(trained_models)}\")\n",
    "    print(f\"trained_models.keys = {trained_models.keys()}\")\n",
    "    return trained_models\n",
    "    \n",
    "#     return (\n",
    "#         data.groupby(iteration_col_name)\n",
    "#         .apply(\n",
    "#             _calibrate_models_per_fold,\n",
    "#             trained_models,\n",
    "#             calibration_method,\n",
    "#             calibration_split_name,\n",
    "#             split_col_name,\n",
    "#         )\n",
    "#         .to_dict()\n",
    "#     )\n",
    "\n",
    "\n",
    "def _calibrate_models_per_fold(partition, trained_models, calibration_method, calibration_split_name, split_col_name):\n",
    "# def _calibrate_models_per_fold(partition: pd.DataFrame, trained_models: Mapping[str, Any], calibration_method: AnyStr, calibration_split_name: AnyStr, split_col_name: AnyStr,) -> Any:\n",
    "    \"\"\"Calibrates a single pretrained model using `CalibratedClassifierCV`.\"\"\"\n",
    "    print(\"\\n\\n\")\n",
    "#     print(f\"calibration_method={calibration_method}\")\n",
    "    iteration_id = partition.name\n",
    "    prefit_model = trained_models[iteration_id]\n",
    "#     print(f\"iteration_id={iteration_id}, prefit_model={prefit_model}\")\n",
    "\n",
    "#     calibrated_model = prefit_model.calibrate(\n",
    "#         partition, calibration_method, calibration_split_name, split_col_name\n",
    "#     )\n",
    "    \n",
    "    # calibrate method from ModelContainer starts!!!! \n",
    "    fitted_model = prefit_model.get('fitted_model')\n",
    "    feature_col_names = prefit_model.get(\"feature_col_names\")\n",
    "    fitted_model = prefit_model.get(\"fitted_model\")\n",
    "#     print(f\"fitted_model = {fitted_model}\")\n",
    "#     print(f\"feature_col_names = {feature_col_names}\")\n",
    "    _check_is_fitted(fitted_model)\n",
    "    \n",
    "    index_cal = partition[split_col_name] == calibration_split_name\n",
    "    data_cal = partition.loc[index_cal].copy()\n",
    "    X_train = data_cal[feature_col_names]\n",
    "    y_train = data_cal[target_col_name]\n",
    "    y_train = y_train.astype(int)\n",
    "#     print(f\"\\n index_cal.shape = {index_cal.shape}\")\n",
    "#     print(f\"data_cal.shape = {data_cal.shape}\")\n",
    "#     print(f\"X_train.shape = {X_train.shape}\")\n",
    "#     print(f\"y_train.shape = {y_train.shape}\")\n",
    "\n",
    "    if isinstance(fitted_model, (imblearn_pipeline, sklearn_pipeline)):\n",
    "        print(\"11111111\")\n",
    "        pipeline = fitted_model\n",
    "        estimator_tuple = pipeline.steps[-1]\n",
    "#         print(f\"estimator_tuple={estimator_tuple}\")\n",
    "        preprocessor =  pipeline.steps[0][1]\n",
    "        calibrator = CalibratedClassifierCV(base_estimator=estimator_tuple[1], cv=\"prefit\", method=calibration_method)\n",
    "        calibrated_model = calibrator.fit(preprocessor.transform(X_train), y_train)\n",
    "        pipeline.steps[-1] = (estimator_tuple[0], calibrator)\n",
    "    else:\n",
    "        print(\"222222222222\")\n",
    "        calibrator = CalibratedClassifierCV(base_estimator=fitted_model, method=calibration_method, cv=\"prefit\")\n",
    "\n",
    "        calibrated_model = calibrator.fit(X_train, y_train)\n",
    "\n",
    "    return calibrated_model\n",
    "#     print(f\"calibrated_model = {calibrated_model}\")\n",
    "#     print(f\"len(trained_models[{iteration_id}]) = {len(trained_models[iteration_id])}, trained_models[{iteration_id}] = {trained_models[iteration_id].keys()}\")\n",
    "#     trained_models[iteration_id]['calibrated_model'] = calibrated_model\n",
    "#     print(\"********************\")\n",
    "#     print(f\"\\n\\nlen(trained_models[{iteration_id}]) = {len(trained_models[iteration_id])}, trained_models[{iteration_id}] = {trained_models[iteration_id].keys()}\")    \n",
    "#     print(\"---------------------------------------\")\n",
    "#     return trained_models\n",
    "\n",
    "\n",
    "def _check_is_fitted(fitted_model):\n",
    "    \"\"\"Checks whether the model has been fitted already.\"\"\"\n",
    "    if fitted_model is None:\n",
    "        raise NotFittedError(\"This estimator has not been fitted yet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Before calibrate_classifiers(): \\nlen(fit_models) = {len(fit_models)}, keys={len(fit_models[0].keys())}, and keys are: \\n {fit_models[0].keys()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calibration_params = {'method': 'sigmoid', 'calibration_split_name': 'CAL'}\n",
    "\n",
    "calibrated_model = calibrate_classifiers(split_train_test_cal, fit_models, splitting_params, calibration_params, target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"After calibrate_classifiers(): \\nlen(fit_models) = {len(fit_models)}, keys={len(fit_models[0].keys())}, and keys are: \\n {fit_models[0].keys()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calibrated_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# node(\n",
    "#     func=calibrate_classifiers,\n",
    "#     inputs={\n",
    "#         \"data\": f\"{catalog_str}.split_train_test_cal\",\n",
    "#         \"trained_models\": f\"{catalog_str}.fit_models\",\n",
    "#         \"splitting_params\": f\"{param_str}.splitting_params\",\n",
    "#         \"calibration_params\": f\"{param_str}.calibration_params\",\n",
    "#     },\n",
    "#     outputs=f\"{catalog_str}.calibrated_model\",\n",
    "#     name=f\"calibrate_model_for_training\",\n",
    "#     tags=[\"training: model\"],\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16- get_model_predictions\n",
    "\n",
    "# <i> Note: calibrated_model is the same as fited_models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_predictions(data, trained_models, splitting_params, inference_params, target_col_name):\n",
    "# def get_model_predictions(data: pd.DataFrame, trained_models: Mapping[str, Any], splitting_params: Mapping[str, Any], inference_params: Mapping[str, Any] = None,) -> pd.DataFrame:\n",
    "    \"\"\"Makes predictions for each fold given a dictionary of trained models.\n",
    "\n",
    "    Predictions are saved into a new column. If the trained model implements a\n",
    "    `predict_proba` method and is a binary classifier, then the prediction\n",
    "    score for the second class is stored in an additional new columns with the name\n",
    "    `target` + `prediction_proba_suffix`. The `prediction_proba_suffix` can be provided\n",
    "    in the `inference_params`.\n",
    "\n",
    "    `inference_params` expects the following key value pairs:\n",
    "        `prediction_suffix`: (Optional) Suffix appended to target to form prediction\n",
    "            column name. Default is `_pred`\n",
    "        `prediction_proba_suffix`: (Optional) Suffix appended to target to form\n",
    "            prediction score name. Default is `_pred_score`\n",
    "\n",
    "    `splitting_params` expects the following key value pairs:\n",
    "        `iteration_col_name`: Name of column in which the fold id is to be saved\n",
    "\n",
    "    Args:\n",
    "        data: Data containing fold id and features\n",
    "        trained_models: A dictionary with fold ids as keys and trained models as values\n",
    "        splitting_params: A dictionary describing how splitting is done, including\n",
    "            in which column the iteration id resides\n",
    "        inference_params: (Optional) Parameters including the suffix to use\n",
    "            for predictions\n",
    "\n",
    "    Returns:\n",
    "        A new dataframe with the predictions as a new column, and the prediction scores\n",
    "        if a `predict_proba` method is available and it's a binary classifier.\n",
    "    \"\"\"\n",
    "    iteration_col_name = splitting_params.get(\"iteration_col_name\", \"iteration_id\")\n",
    "\n",
    "    inference_params = inference_params if inference_params else {} # {'prediction_suffix': '_pred'}\n",
    "\n",
    "    prediction_suffix = inference_params.get(\"prediction_suffix\", \"_pred\") # '_pred'\n",
    "    prediction_proba_suffix = inference_params.get(\"prediction_proba_suffix\", \"_pred_score\") # '_pred_score'\n",
    "    \n",
    "\n",
    "    print(f\"  iteration_col_name={iteration_col_name}, \\n  inference_params={inference_params}\\n  prediction_suffix={prediction_suffix} \\n  prediction_proba_suffix={prediction_proba_suffix} \\n\")\n",
    "    \n",
    "    return data.groupby(iteration_col_name).apply(\n",
    "        _make_predictions_per_fold,\n",
    "        trained_models,\n",
    "        prediction_suffix,\n",
    "        prediction_proba_suffix,\n",
    "    )\n",
    "\n",
    "\n",
    "def _make_predictions_per_fold(partition, trained_models, prediction_suffix, prediction_proba_suffix):\n",
    "# def _make_predictions_per_fold(partition: pd.DataFrame, trained_models: Mapping[str, Any], prediction_suffix: str, prediction_proba_suffix: str,):\n",
    "    \"\"\"Makes predictions for one fold.\n",
    "\n",
    "    If the model implements a `predict_proba` method and is a binary\n",
    "    classifier, then the prediction score is calculated, too.\n",
    "    \"\"\"\n",
    "#     print(\"\\n\\n****************\")\n",
    "#     print(f\"trained_models: {trained_models}\")\n",
    "    iteration_id = partition.name # 0\n",
    "#     print(f\"iteration_id={iteration_id}\")\n",
    "    model = trained_models[iteration_id]\n",
    "#     print(f\"model.keys = {model.keys()}\")\n",
    "\n",
    "    target_col_name = model.get('target_col_name')\n",
    "#     print(f\"\\ntarget_col_name={target_col_name}\")\n",
    "    \n",
    "    pred_col_name = target_col_name + prediction_suffix\n",
    "    pred_score_col_name = (\n",
    "        target_col_name + prediction_proba_suffix\n",
    "        if hasattr(model.get(\"fitted_model\"), \"predict_proba\")\n",
    "        else None\n",
    "    ) # 'tgt_xsell_cust_voice_to_fixed_pred_score'\n",
    "\n",
    "#     print(f\"pred_col_name={pred_col_name}, \\npred_score_col_name={pred_score_col_name}\")\n",
    "    \n",
    "    # predict model from ModelContainer starts!!! \n",
    "#     partition = model.predict(partition, pred_col_name, pred_score_col_name)\n",
    "#         _check_is_fitted(self.fitted_model)\n",
    "\n",
    "    if (pred_col_name is None) and (pred_score_col_name is None):\n",
    "        raise TypeError(\n",
    "            \"Expected pred_col_name or pred_score_col_name, neither was given.\"\n",
    "        )\n",
    "\n",
    "    if pred_col_name is not None:\n",
    "#         print(\"1111111111\")\n",
    "#         print(f\"len feature_col_names = {len(model.get('feature_col_names'))}\")\n",
    "        prediction = model.get(\"fitted_model\").predict(partition[model.get(\"feature_col_names\")]) # len(self.feature_col_names) = 160, rediction.shape = (326297,)\n",
    "        partition[pred_col_name] = prediction # (326297, 166)\n",
    "#         print(f\"prediction = {prediction}\")\n",
    "\n",
    "    if pred_score_col_name is not None:\n",
    "\n",
    "        if not hasattr(model.get(\"fitted_model\"), \"predict_proba\"):\n",
    "#             print(\"22222222222\")\n",
    "            raise AttributeError(\n",
    "                f\"A pred_score_col_name was passed, but {type(model.get('fitted_model'))} \"\n",
    "                + \"has no attribute predict_proba. This indicates that the model \"\n",
    "                + \"is not a classifier and can not predict a score. \"\n",
    "                + \"Remove the pred_score_col_name argument or change \"\n",
    "                + \"the model to a classifier.\"\n",
    "            )\n",
    "\n",
    "        prediction_score = model.get(\"fitted_model\").predict_proba(partition[model.get(\"feature_col_names\")]) # (326297, 2)\n",
    "#         print(f\"\\nprediction_score.shape = {prediction_score.shape}\")\n",
    "        if len(model.get(\"fitted_model\").classes_) > 2:\n",
    "#             print(\"3333333333333333333\")\n",
    "            class_score_names = [\n",
    "                pred_score_col_name + \"_\" + str(category)\n",
    "                for category in model.get(\"fitted_model\").classes_\n",
    "            ]\n",
    "            for (i, class_score_name) in enumerate(class_score_names):\n",
    "                partition[class_score_name] = prediction_score[:, i]\n",
    "\n",
    "        else:\n",
    "#             print(\"44444444444444444444\")\n",
    "            partition[pred_score_col_name] = prediction_score[:, -1] # goes to else!\n",
    "\n",
    "#     print(f\"partition.shape={partition.shape}\")\n",
    "    return partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calibrated_model[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_params = {'prediction_suffix': '_pred'}\n",
    "\n",
    "# calibrated_model is the same as fited_models\n",
    "\n",
    "predictions = get_model_predictions(split_train_test_cal, calibrated_model, splitting_params, inference_params, target)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# node(\n",
    "#     func=get_model_predictions,\n",
    "#     inputs={\n",
    "#         \"data\": f\"{catalog_str}.split_train_test_cal\",\n",
    "#         \"trained_models\": f\"{catalog_str}.calibrated_model\",\n",
    "#         \"splitting_params\": f\"{param_str}.splitting_params\",\n",
    "#         \"inference_params\": f\"{param_str}.inference_params\",\n",
    "#     },\n",
    "#     outputs=f\"{catalog_str}.predictions\",\n",
    "#     name=f\"prediction_for_training\",\n",
    "#     tags=[\"training: predictions\"],\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 17- get_model_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, balanced_accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "\n",
    "# pylint: disable=too-many-locals\n",
    "def get_model_performance(data, metrics_params, splitting_params, target_col_name, inference_params):\n",
    "# def get_model_performance(data: pd.DataFrame, metrics_params: Mapping[str, Any], splitting_params: Mapping[str, Any], target_col_name: str, inference_params: Mapping[str, Any] = None,) -> pd.DataFrame:\n",
    "    \"\"\"Get model performance.\n",
    "\n",
    "    Saves train/test metrics specified in the parameters file, for individual\n",
    "    folds and overall to PAI and returns them in a dictionary.\n",
    "\n",
    "    `metrics_params` expects the following key value pairs:\n",
    "        `metrics`: sklearn performance metrics\n",
    "        `tracker`: tracker name for logging metrics\n",
    "\n",
    "    `splitting_params` expects the following key value pairs:\n",
    "        `iteration_col_name`: Name of column in which the fold id is to be saved\n",
    "        `split_col_name`: name of column in which train/test split is given\n",
    "\n",
    "    Args:\n",
    "        data: Dataframe containing features, targets, predictions, fold and\n",
    "            split information\n",
    "        metrics_params: contains information about metrics to compute and the tracker\n",
    "            for logging the metrics.\n",
    "        splitting_params: Describes where fold and splitting info are located\n",
    "        target_col_name: Name of target column\n",
    "        inference_params: (Opt) Contains the prediction suffixes.\n",
    "\n",
    "    Returns:\n",
    "        Dictionary containing train and test metrics for each fold and for\n",
    "        overall data.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    iteration_col_name = splitting_params.get(\"iteration_col_name\", \"iteration_id\")\n",
    "    split_col_name = splitting_params.get(\"split_col_name\", \"split\")\n",
    "\n",
    "    inference_params = inference_params if inference_params else {}\n",
    "\n",
    "    prediction_suffix = inference_params.get(\"prediction_suffix\", \"_pred\")\n",
    "    prediction_proba_suffix = inference_params.get(\n",
    "        \"prediction_proba_suffix\", \"_pred_score\"\n",
    "    )\n",
    "\n",
    "    prediction_col_name = target_col_name + prediction_suffix\n",
    "    pred_score_col_name = target_col_name + prediction_proba_suffix\n",
    "\n",
    "    model_metrics = {}\n",
    "    scorers = []\n",
    "\n",
    "    metric_params = metrics_params[\"metrics\"]\n",
    "    tracker_name = metrics_params.get(\"tracker\")\n",
    "    \n",
    "#     print(f\"\\titeration_col_name= {iteration_col_name}, \\n\\tsplit_col_name = {split_col_name}, \\n\\tinference_params={inference_params},  \\n\\tprediction_suffix={prediction_suffix}, \\n\\tprediction_proba_suffix={prediction_proba_suffix} , \\n\\tpred_score_col_name={pred_score_col_name}, \\n\\tmetric_params={metric_params}, \\n\\ttracker_name={tracker_name}\")\n",
    "    \n",
    "    \"\"\"Computes the value of the metric for train and test sets.\"\"\"\n",
    "    train_index = data[split_col_name] == \"TRAIN\"\n",
    "    test_index = data[split_col_name] == \"TEST\"\n",
    "\n",
    "    y_train = data.loc[train_index, target_col_name]\n",
    "    y_pred_train = data.loc[train_index, prediction_col_name]\n",
    "\n",
    "    y_test = data.loc[test_index, target_col_name]\n",
    "    y_pred_test = data.loc[test_index, prediction_col_name]\n",
    "    \n",
    "\n",
    "    scorer_name = 'roc_auc_score'\n",
    "#     print(f\"\\n--------------------- {scorer_name}\")\n",
    "#     roc_auc_score(y_true, y_score, *, average='macro', sample_weight=None, max_fpr=None, multi_class='raise', labels=None)\n",
    "    train_metric_value_roc = roc_auc_score(y_train.values.astype(int), y_pred_train.values.astype(int))\n",
    "    test_metric_value_roc = roc_auc_score(y_test.values.astype(int), y_pred_test.values.astype(int))\n",
    "#     print(f\"train_metric_value={train_metric_value_roc}, \\t test_metrics_value={test_metric_value_roc}\")\n",
    "    scorers.append(f\"train_{scorer_name}\")\n",
    "    scorers.append(f\"test_{scorer_name}\")\n",
    "    model_metrics = _append_float_metric(model_metrics,train_metric_value_roc,test_metric_value_roc,target_col_name,\"overall\",scorer_name)\n",
    "    \n",
    "    \n",
    "    scorer_name = 'balanced_accuracy_score'\n",
    "#     print(f\"\\n--------------------- {scorer_name}\")\n",
    "#     balanced_accuracy_score(y_true, y_pred, *, sample_weight=None, adjusted=False)\n",
    "    train_metric_value_balanced = balanced_accuracy_score(y_train.values.astype(int), y_pred_train.values.astype(int))\n",
    "    test_metric_value_balanced = balanced_accuracy_score(y_test.values.astype(int), y_pred_test.values.astype(int))\n",
    "#     print(f\"train_metric_value={train_metric_value_balanced}, \\t test_metric_value={test_metric_value_balanced}\")\n",
    "    scorers.append(f\"train_{scorer_name}\")\n",
    "    scorers.append(f\"test_{scorer_name}\")\n",
    "    model_metrics = _append_float_metric(model_metrics,train_metric_value_balanced,test_metric_value_balanced,target_col_name,\"overall\",scorer_name)\n",
    "  \n",
    "\n",
    "    scorer_name = 'f1_score'\n",
    "#     print(f\"\\n--------------------- {scorer_name}\")\n",
    "#     f1_score(y_true, y_pred, *, labels=None, pos_label=1, average='binary', sample_weight=None, zero_division='warn')\n",
    "    train_metric_value_f1 = f1_score(y_train.values.astype(int), y_pred_train.values.astype(int))\n",
    "    test_metric_value_f1 = f1_score(y_test.values.astype(int), y_pred_test.values.astype(int))\n",
    "#     print(f\"train_metric_value={train_metric_value_f1}, \\t test_metric_value={test_metric_value_f1}\")\n",
    "    scorers.append(f\"train_{scorer_name}\")\n",
    "    scorers.append(f\"test_{scorer_name}\")\n",
    "    model_metrics = _append_float_metric(model_metrics,train_metric_value_balanced,test_metric_value_balanced,target_col_name,\"overall\",scorer_name)\n",
    "    \n",
    "    \n",
    "    scorer_name = 'precision_score'\n",
    "#     print(f\"\\n--------------------- {scorer_name}\")\n",
    "#     precision_score(y_true, y_pred, *, labels=None, pos_label=1, average='binary', sample_weight=None, zero_division='warn')\n",
    "    train_metric_value_precision_score = precision_score(y_train.values.astype(int), y_pred_train.values.astype(int))\n",
    "    test_metric_value_precision_score = precision_score(y_test.values.astype(int), y_pred_test.values.astype(int))\n",
    "#     print(f\"train_metric_value={train_metric_value_precision_score}, \\t test_metric_value={test_metric_value_precision_score}\")\n",
    "    scorers.append(f\"train_{scorer_name}\")\n",
    "    scorers.append(f\"test_{scorer_name}\")\n",
    "    model_metrics = _append_float_metric(model_metrics,train_metric_value_balanced,test_metric_value_balanced,target_col_name,\"overall\",scorer_name)\n",
    "\n",
    "    \n",
    "    scorer_name = 'recall_score'\n",
    "#     print(f\"\\n--------------------- {scorer_name}\")\n",
    "#     recall_score(y_true, y_pred, *, labels=None, pos_label=1, average='binary', sample_weight=None, zero_division='warn')\n",
    "    train_metric_value_recall_score = recall_score(y_train.values.astype(int), y_pred_train.values.astype(int))\n",
    "    test_metric_value_recall_score = recall_score(y_test.values.astype(int), y_pred_test.values.astype(int))\n",
    "#     print(f\"train_metric_value={train_metric_value_recall_score}, \\t test_metric_value={test_metric_value_recall_score}\")\n",
    "    scorers.append(f\"train_{scorer_name}\")\n",
    "    scorers.append(f\"test_{scorer_name}\")\n",
    "    model_metrics = _append_float_metric(model_metrics,train_metric_value_balanced,test_metric_value_balanced,target_col_name,\"overall\",scorer_name)\n",
    "    \n",
    "    \n",
    "    scorer_name = 'confusion_matrix'\n",
    "#     print(f\"\\n--------------------- {scorer_name}\")\n",
    "#     confusion_matrix(y_true, y_pred, *, labels=None, sample_weight=None, normalize=None)\n",
    "    train_metric_value_confusion_matrix = confusion_matrix(y_train.values.astype(int), y_pred_train.values.astype(int))\n",
    "    test_metric_value_confusion_matrix = confusion_matrix(y_test.values.astype(int), y_pred_test.values.astype(int))\n",
    "#     print(f\"train_metric_value={train_metric_value_confusion_matrix}, \\t test_metric_value={test_metric_value_confusion_matrix}\")\n",
    "    score_names = [\"true_negative\",\"false_positive\",\"false_negative\",\"true_positive\",]\n",
    "    for name in score_names:\n",
    "        scorers.append(f\"train_{name}\")\n",
    "        scorers.append(f\"test_{name}\")\n",
    "    model_metrics = _append_confusion_matrix(model_metrics,train_metric_value_confusion_matrix, test_metric_value_confusion_matrix, target_col_name,\"overall\",)\n",
    "\n",
    "    print(f\"\\n\\nmodel_metrics = {model_metrics['overall_tgt_xsell_cust_voice_to_fixed']}\")\n",
    "    print(f\"\\nscorers = {scorers}\")\n",
    "    print(f\"\\n\\nmodel_metrics = {len(model_metrics['overall_tgt_xsell_cust_voice_to_fixed'])}\")\n",
    "    print(f\"\\nscorers = {len(scorers)}\")    \n",
    "\n",
    "#     result = (pd.DataFrame.from_dict(model_metrics, orient=\"index\", columns=scorers)\n",
    "#               .reset_index()\n",
    "#               .rename(columns={\"index\": \"group\"}))\n",
    "# #     tracking.get_tracker(tracker_name).log_artifacts({\"results\": result})\n",
    "#     return result\n",
    "    return \"code is not complete!! \"\n",
    "\n",
    "\n",
    "def _append_float_metric(metrics_dict, train_metric_value, test_metric_value, target_col_name, key_name, scorer_name,):\n",
    "    metrics_dict = _add_value_to_dict(\n",
    "        train_metric_value, f\"{key_name}_{target_col_name}\", metrics_dict\n",
    "    )\n",
    "    metrics_dict[f\"{key_name}_{target_col_name}\"].append(test_metric_value)\n",
    "\n",
    "    return metrics_dict\n",
    "\n",
    "\n",
    "def _add_value_to_dict(value, key_name, metric_dict):\n",
    "    \"\"\"Checks if a key is in the dictionary and adds the required value.\"\"\"\n",
    "    if key_name in metric_dict:\n",
    "        metric_dict[key_name].append(value)\n",
    "    else:\n",
    "        metric_dict[key_name] = [value]\n",
    "\n",
    "    return metric_dict\n",
    "\n",
    "def _append_confusion_matrix( metrics_dict, train_metric_value, test_metric_value, target_col_name, key_name):\n",
    "    if len(train_metric_value) == 2:\n",
    "    # if train_metric_value.shape[0] == 2:\n",
    "        i =0 \n",
    "        for train_value, test_value in zip(\n",
    "            # train_metric_value.ravel(), test_metric_value.ravel()\n",
    "            train_metric_value, test_metric_value\n",
    "        ):\n",
    "#             print(f\"i={i}\")\n",
    "#             i = i + 1\n",
    "#             print(\"train_value\", train_value)\n",
    "#             print(\"test_value\", test_value)\n",
    "            metrics_dict = _add_value_to_dict(\n",
    "                train_value, f\"{key_name}_{target_col_name}\", metrics_dict\n",
    "            )\n",
    "            metrics_dict[f\"{key_name}_{target_col_name}\"].append(test_value)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"Performance table doesn't support confusion matrices for multi-class \"\n",
    "            \"classification\"\n",
    "        )\n",
    "\n",
    "    return metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_params = {'tracker': 'exp', \n",
    "                  'metrics': ['sklearn.metrics.roc_auc_score', \n",
    "                              'sklearn.metrics.balanced_accuracy_score', \n",
    "                              'sklearn.metrics.f1_score', \n",
    "                              'sklearn.metrics.precision_score', \n",
    "                              'sklearn.metrics.recall_score', \n",
    "                              'sklearn.metrics.confusion_matrix']}\n",
    "inference_params = {'prediction_suffix': '_pred'}\n",
    "\n",
    "performance = get_model_performance(predictions, metrics_params, splitting_params, target, inference_params)\n",
    "performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# node(\n",
    "#     func=get_model_performance,\n",
    "#     inputs={\n",
    "#         \"data\": f\"{catalog_str}.predictions\",\n",
    "#         \"metrics_params\": f\"{param_str}.metrics_params\",\n",
    "#         \"splitting_params\": f\"{param_str}.splitting_params\",\n",
    "#         \"target_col_name\": f\"{param_str}.target\",\n",
    "#         \"inference_params\": f\"{param_str}.inference_params\",\n",
    "#     },\n",
    "#     outputs=f\"{catalog_str}.performance\",\n",
    "#     name=f\"performance_for_training\",\n",
    "#     tags=[\"training: performance\"],\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def to_pandas(data, pandas_params, spine_params, target_col, data_dictionary, features, ref_date):\n",
    "# def to_pandas(data: DataFrame, pandas_params: Dict[str, Any], spine_params: Dict[str, Any], target_params: Dict[str, Any], data_dictionary: Optional[DataDict] = None, features: List[str] = None, ref_date: Optional[str] = None,) -> pd.DataFrame:\n",
    "    ref_period = pandas_params.get(\"ref_period\")\n",
    "    include_target = pandas_params.get(\"include_target\")\n",
    "    key_col = to_list(spine_params.get('keys'))\n",
    "    date_col = spine_params.get('date_column')\n",
    "#     target_col = target_params.get('target_variable_column')\n",
    "\n",
    "    if ref_date and ref_period:\n",
    "        raise AttributeError(\n",
    "            \"Cannot specify both ref_date and ref_period at\"\n",
    "            \"the same time. Specify only one argument.\"\n",
    "        )\n",
    "        \n",
    "    print(f\"\\tref_period={ref_period}, \\n\\tinclude_target={include_target}, \\n\\tkey_col={key_col}, \\n\\tdate_col={date_col}, \\n\\ttarget_col={target_col}, \\n\\tdata_dictionary={data_dictionary}\")\n",
    "    print(f\"len(features)= {len(features)}\")\n",
    "    \n",
    "    feat_col, target_switch_col = [None] * 2\n",
    "    \n",
    "    if data_dictionary or features:\n",
    "        if data_dictionary:\n",
    "            feat_col = data_dictionary.get_features()\n",
    "            target_switch_col = data_dictionary.get_target_switch_variable()\n",
    "        elif features:\n",
    "            feat_col = features\n",
    "\n",
    "        export_cols = []\n",
    "        if date_col:\n",
    "            export_cols += [date_col]\n",
    "        if key_col:\n",
    "            export_cols += key_col\n",
    "        export_cols += feat_col\n",
    "        if include_target and target_col:\n",
    "            export_cols += [target_col]\n",
    "\n",
    "        cols_diff = set(export_cols) - set(data.columns)\n",
    "        if len(cols_diff) != 0:\n",
    "            # logger.warning(f\"The following columns are not available in the DataFrame and will be ignored: {cols_diff}\")\n",
    "            export_cols = list(set(export_cols) - cols_diff)\n",
    "\n",
    "        print(f\"len(export_cols)={len(export_cols)}\")\n",
    "        print(f\"Before: {data.shape}\")\n",
    "        data = data[export_cols]\n",
    "        print(f\"After: {data.shape}\")        \n",
    "\n",
    "    if ref_date and date_col:\n",
    "        try:\n",
    "            if not re.match(r\"\\d{4}-\\d{2}-\\d{2}\", ref_date):\n",
    "                raise ValueError(\"Date format should follow yyyy-mm-dd.\")\n",
    "\n",
    "            ref_date = pd.to_datetime(ref_date, infer_datetime_format=True).strftime(\"%Y-%m-%d\")\n",
    "            print(f\"ref_date={ref_date}, type = {type(ref_date)}\")\n",
    "            \n",
    "            data['dt'] = pd.to_datetime(data[date_col], format='%Y-%m-%d %H:%M:%S')\n",
    "            data = data[data['dt'] == ref_date]\n",
    "            print(f\"After filter for day {ref_date} data.shape = {data.shape}\")            \n",
    "            data = data.drop(columns=['dt'])\n",
    "            print(f\"After drop filter for day {ref_date} data.shape = {data.shape}\")\n",
    "        except (ValueError, TypeError) as exception:\n",
    "            raise exception(\"'ref_date' type casting failed\") from exception\n",
    "\n",
    "    # TODO: Sarah The followings are not tested!! \n",
    "    print(f\"first if = {include_target and target_col}, 2nd if={include_target and target_switch_col}, 3rd if={ref_period and date_col}\")\n",
    "    if include_target and target_col:\n",
    "        print(f\"\\nBefore include_target shape is {data.shape}\")\n",
    "        data = data[data[target_col.notnull()]]\n",
    "        print(f\"After include_target shape is {data.shape}\")\n",
    "\n",
    "    if include_target and target_switch_col:\n",
    "        print(f\"\\nBefore include_target and target_switch_col shape is {data.shape}\")\n",
    "        # data = data.filter(~f.col(target_switch_col))\n",
    "        data = data[data[target_switch_col]]\n",
    "        print(f\"After include_target and target_switch_col shape is {data.shape}\")\n",
    "                    \n",
    "    if ref_period and date_col:\n",
    "        print(f\"\\nBefore filter_ref_window shape is {data.shape}\")\n",
    "        # data = filter_ref_window(data, ref_period, date_col)\n",
    "        print(f\"After filter_ref_window shape is {data.shape}\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def to_list(item):\n",
    "    \"\"\"\n",
    "    Wraps an item to a list or returns the existing list\n",
    "\n",
    "    Args:\n",
    "        item (Union[Any, List[Any]]): a list of a single object\n",
    "\n",
    "    Returns:\n",
    "        item wrapped in a list, if item is not a list. Otherwise the\n",
    "        original list is returned\n",
    "    \"\"\"\n",
    "\n",
    "    return [item] if not isinstance(item, list) else item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_params = {'include_target': False}\n",
    "spine_params = {'keys': ['customer_id'], \n",
    "                'date_column': 'current_dt', \n",
    "                'product_holdings_filter': {'exact_match': {'product_category': 'fixedbroadband'}}, \n",
    "                'is_deepsell': 'N'}\n",
    "ref_date = '2021-06-30'\n",
    "data_dictionary = None\n",
    "inference_master_table = to_pandas(df, pandas_params, spine_params, target, data_dictionary, features, ref_date)\n",
    "inference_master_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# node(\n",
    "#     func=spark_utils.to_pandas,\n",
    "#     inputs={\n",
    "#         \"data\": f\"{model_name}.master.mst_tbl\",\n",
    "#         \"pandas_params\": f\"{param_str}.inference_spark_to_pandas\",\n",
    "#         \"spine_params\": f\"params:{model_name}.master.spine\",\n",
    "#         \"target_params\": f\"params:{model_name}.master.target\",\n",
    "#         \"features\": f\"{model_name}.in_memory.features_to_export\",\n",
    "#         \"ref_date\": \"params:ref_date\",\n",
    "#     },\n",
    "#     outputs=f\"{model_name}.in_memory.inference_master_table\",\n",
    "#     name=f\"master_table_to_pandas_for_inference\",\n",
    "#     tags=[\"inference: CDL to edge\"],\n",
    "# ),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# attach_live_iter_split_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attach_live_iter_split_col(data, splitting_params):\n",
    "# def attach_live_iter_split_col(data: pd.DataFrame, splitting_params: Mapping[str, Any]) -> pd.DataFrame:\n",
    "    \"\"\"Adds fold and split columns to any dataframe.\n",
    "\n",
    "    The fold is set to 0, split is set to \"LIVE\". This is useful for live data from\n",
    "    e.g. a UI.\n",
    "\n",
    "    `splitting_params` expects the following keys:\n",
    "        `iteration_col_name`: Name of column in which the fold id is to be saved\n",
    "        `split_col_name`: Name of column in which the TRAIN/TEST split is saved\n",
    "\n",
    "    Args:\n",
    "        data: Model input data containing spine, features and target\n",
    "        splitting_params: Dictionary with splitting configuration\n",
    "\n",
    "    Returns:\n",
    "        A new pandas dataframe with a fold and split column attached\n",
    "    \"\"\"\n",
    "    iteration_col_name = splitting_params.get(\"iteration_col_name\", \"iteration_id\")\n",
    "    split_col_name = splitting_params.get(\"split_col_name\", \"split\")\n",
    "\n",
    "    data[iteration_col_name] = 0\n",
    "    data[split_col_name] = \"LIVE\"\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting_params = {'splitter': 'customerone.pipelines.telco.test_train_splitter.OutOfTimeSplitter', \n",
    "#                     'splitter_args': {'n_splits': 1, \n",
    "#                                       'test_size': 0.2, \n",
    "#                                       'date_col': 'current_dt', \n",
    "#                                       'verbose': 1, \n",
    "#                                       'random_state': 42}, \n",
    "#                     'group_col_name': 'customer_id', \n",
    "#                     'iteration_col_name': 'iteration_id', \n",
    "#                     'split_col_name': 'split'}\n",
    "\n",
    "inference_master_table_live = attach_live_iter_split_col(inference_master_table, splitting_params)\n",
    "inference_master_table_live"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# node(\n",
    "#     func=attach_live_iter_split_col,\n",
    "#     inputs={\n",
    "#         \"data\": f\"{model_name}.in_memory.inference_master_table\",\n",
    "#         \"splitting_params\": f\"{param_str}.splitting_params\",\n",
    "#     },\n",
    "#     outputs=f\"{model_name}.inference.inference_master_table\",\n",
    "#     name=f\"add_iteration_id\",\n",
    "#     tags=[\"inference: CDL to edge\"],\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# inference predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = get_model_predictions(split_train_test_cal, calibrated_model, splitting_params, inference_params, target)\n",
    "\n",
    "\n",
    "get_model_predictions(inference_master_table_live, calibrated_model, splitting_params, inference_params, target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                node(\n",
    "                    func=get_model_predictions,\n",
    "                    inputs={\n",
    "                        \"data\": f\"{model_name}.inference.inference_master_table\",\n",
    "                        \"trained_models\": f\"{model_name}.in_memory.inf_model\",\n",
    "                        \"splitting_params\": f\"{param_str}.splitting_params\",\n",
    "                        \"inference_params\": f\"{param_str}.inference_params\",\n",
    "                    },\n",
    "                    outputs=f\"{model_name}.in_memory.inf_predictions\",\n",
    "                    name=f\"prediction\",\n",
    "                    tags=[\"inference: predictions\"],\n",
    "                ),\n",
    "                node(\n",
    "                    func=add_run_id,\n",
    "                    inputs={\n",
    "                        \"data\": f\"{model_name}.in_memory.inf_predictions\",\n",
    "                        \"run_id\": f\"{model_name}.training.run_id\",\n",
    "                    },\n",
    "                    outputs=f\"{model_name}.inference.inf_predictions\",\n",
    "                    name=\"add_run_id_to_predictions\",\n",
    "                    tags=[\"inference: predictions\"],\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-north-1:243637512696:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
